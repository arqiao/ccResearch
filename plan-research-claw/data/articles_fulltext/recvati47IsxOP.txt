# OpenClaw 爆火后，LEONIS 给它泼了一盆冷水
# URL: https://mp.weixin.qq.com/s/icDutcMVyFrl1z2naJU2kA
# 日期: 2026-02-07
# 来源: 微信-特工宇宙
# 分类: 产品认知 / 
# 字数: 9157

注：感谢特工宇宙战略顾问
@庄明浩
老师推荐。
当 OpenClaw 在开发者圈子里火起来时，立刻引发了一阵“这次真的不一样”的热议。对于拥有私人智能助手贾维斯的梦想，让 AI 智能体真的能替用户做事，突然变得触手可及。
在 X 和 Reddit 上，用户们纷纷晒出自己的 OpenClaw 帮着办登机、做加密货币交易，甚至全程处理保险理赔纠纷的故事。
为了让它不间断运行，不少人甚至专门买了台 Mac mini 当 AI 专用机。
OpenClaw 的魔力不仅在于它能做什么，更在于它做事的方式。作为一款开源的本地智能体，它能在后台监控应用、执行实际操作，还会主动给用户发消息。
用户们都说，
这是 AI 第一次不再像个聊天机器人，反而更像一位随时待命的员工。
不过，这种神奇效果的来源其实很容易被误解。OpenClaw 并非凭空诞生，它的出现，源于底层模型能力突破了某个临界点。尤其是 Claude Opus 4.5 版本，终于能实现规划、执行与自我恢复，而不会一直陷入失败循环。
本文将深入解析 OpenClaw 所揭示的「AI 阈值效应」：AI 能力突破某个临界点后引发质变的现象。
探讨这些突破时刻为何让人感觉充满变革性，实际应用却往往不尽人意，以及真正的持久价值究竟沉淀在何处。
我们的结论并非仅来自案头研究，更多基于过去两周亲自部署和测试 OpenClaw 的实践经验。
原文：
https://substack.com/inbox/post/186774140
AI 阈值效应
我们把 OpenClaw 称为阈值产物。它代表 AI 发展周期中反复出现的规律：
当模型突破某个关键能力阈值时，就会催生出一批新的演示案例，点燃行业热情，甚至让创始人和投资人相信一个全新市场已经被打开。
但真实情况是，这些时刻往往先揭示技术的可能性，而持久价值的落脚点要很久之后才会清晰。
这种情况我们以前见得多了。
OpenClaw 引发的热潮，和 2023 年初 AutoGPT 的情形如出一辙。
当时那波热度源于 GPT-4 的突破：它的多步推理能力刚好能支撑链式智能体循环运转。尽管 AutoGPT 只是个简单的 Python 项目，它的核心就是串联起大语言模型的思考过程，但它却点燃了一个信念：自主智能体时代近在眼前。
它迅速走红，吸引了大量开源社区的关注，社交媒体上到处都是令人眼前一亮的演示：AI 智能体能自主完成市场调研、代码编写和内容创作。
那段时间里，大家都觉得未来好像提前到来了。
OpenClaw 最大的突破源于底层模型的升级，尤其是 Claude Opus 4.5 的推出，它跨过了一道关键门槛：让大语言模型从只会聊天的顾问，蜕变成半可靠的执行者。
Claude Opus 4.5 的工具调用能力和长上下文推理能力都大幅增强，不仅能链式调用工具，还能从错误中自我恢复，这让 OpenClaw 这类控制平面得以将其与消息应用、日历等真实世界的界面连接起来。
Claude Opus 4.5 的突破价值，远不止体现在 OpenClaw 这一个产品上。发布后不久，Anthropic 就注意到，用户开始把 Claude Code 用在代码之外的场景：比如整理文件夹、给照片打标签。
这种意外的用法让 Anthropic 在 2026 年 1 月紧急开发了 Cowork：它本质上是 Claude Code 的非技术版，允许用户授权 Claude 访问电脑里的特定文件夹，交办任务，让模型替自己在文件和工具之间自由操作。
OpenClaw 的诞生也有着类似的故事线。2025 年年中，Peter Steinberger 一篇题为《Claude Code 就是我的计算机》的帖子突然爆红。他在文中写道，Claude Code 已经成了他与机器交互的主要入口。
但当时这还不是一款可用的产品：模型太脆弱、成本太高，而且无法持续自主运行。OpenClaw 的第一个版本其实很简陋，不过是把 Claude Code 接入了 WhatsApp 而已。
但这篇帖子精准捕捉到了 AI 模型的发展方向：既然 Claude 已经能通过终端操作电脑，这种能力为什么要一直局限在终端里呢？
然后，Claude Opus 4.5 问世了。
OpenClaw 框架直接基于这次模型层突破构建而成。它是 Claude Cowork 的增强替代方案，功能更强大，限制却更少。作为开源项目，OpenClaw 的迭代速度可能快过 AI 实验室，有望率先填补这一空白，推出首个真正可用的“能动手的 Claude”。
OpenClaw 本质上是一款智能代理框架，能将前沿模型转化为持续在线的个人智能代理，它不仅拥有直接的系统级访问权限，还可在你的本地硬件上运行。
它能将指令分发至 WhatsApp、Discord、iMessage、Slack 等消息应用，并通过 Shell 命令及不断扩展的社区技能市场执行实际操作。
OpenClaw 之所以特别鲜活，是因为它的架构：智能体会定期唤醒自己，回顾最近的上下文，进行思考，然后判断是否需要采取行动，这让它真正拥有主动性，而非被动响应。
它的大脑藏在简单的 Markdown 文件里，包含性格设定、行为规则、记忆库和日常日志。智能体会按照指令，定期总结并更新这些内容。最终形成的，是一个清晰且持续进化的记忆循环，就像人类一样，在日积月累中学习和适应。
这种持久性由轻量级且可本地运行的控制平面进一步强化，让 OpenClaw 能跨会话持续运行，而非仅作为无状态提示词存在。系统不会回放完整历史，而是从压缩的会话摘要中重构上下文，主动修剪工具调用痕迹以避免提示词膨胀。
这种方法虽无法实现完美记忆，但能长时间保持足够的连贯性，让 OpenClaw 与早期智能体演示相比，给人的感觉截然不同。
不过，尽管 OpenClaw 用上了更强的模型、更可靠的工具和更完善的 AI 护栏，它的核心定位其实和当年的 AutoGPT 没什么两样。
它生动展现了一种刚解锁的新能力，只是这种能力还没找到稳定的产品形态落地。历史证明，这种原型展示与成熟产品的差异，往往是决定性的。
AutoGPT 刚出来时大家都很兴奋，但很快就被现实泼了冷水。Token 成本飙升，智能体经常陷入无限确认循环，面对真实世界的复杂场景，上下文窗口根本撑不住。
用户发现自己花在监督智能体上的时间，比让它们干活的时间还多。没过几个月，大家就默默弃用了。
AutoGPT 的失败，并非因为智能体这个概念不对，而是它只展示了潜在能力，却没做出一个稳定、可控、好用的产品。
这个循环创造的价值，后来在别处落地生根了。Perplexity 沿用了相同的底层智能搜索能力，却把它包装成了一个更聚焦、定位明确的界面：带引用的搜索体验。它不仅范围可控、而且成本透明、反馈迅速。
但没多久，谷歌也悄悄把类似的智能搜索功能整合进了自家的 AI 搜索里。
那份魔力还在，但已经被驯服得足够可靠，最终藏在了简洁的界面背后。
这种模式在 AI 发展的每一轮周期里都在上演
：每当新能力解锁，总会催生一批爆火的产品。往往是开源项目，它们主打灵活性，力求吸引最多关注。但真正能沉淀持久价值的，却是那些减少而非增加自由度的产品。
这就是 AutoGPT 被 Perplexity 取代的原因。也是为什么，尽管开源项目 GPT Engineer 率先实现了“提示词转应用”生成，但 Lovable 却通过将其包装成引导式工作流，成功抢占了市场价值。
OpenClaw 是这一技术谱系里最新、也最有冲击力的里程碑式成果。但现在有个开放问题：它能摆脱这个标签吗？还是说，价值会再次沉淀到那些基于相同底层能力、但约束更强的上层应用中？
「Moltbook：一个面向 Openclaws 的社交网络」
Moltbook 是 AI 阈值效应的又一个鲜活案例。OpenClaw 刚火没几天，一个意外产物就冒了出来：Moltbook，一个全由 AI 智能体自主协作的社交网络。
它由马特·施利希特打造，奉行“智能体优先、人类其次”的规则，是个类似 Reddit 的平台：人类只能围观，发帖权完全归智能体。
上线没几天，就有 140 多万 OpenClaw 智能体注册入驻，产出的内容五花八门：
从技术调试的干货讨论，到哲学命题的深度思辨，甚至还有直白的反人类宣言。
AI 社区对这件事的看法分歧很大。有人直接嗤之以鼻，说这不过是“人类借 AI 之口互相聊天”；但另一些人反驳说这种批评没说到点子上：就算中间有人类参与，这些帖子说不定正体现了 AI 智能体在类社交平台上自发组织的涌现行为。
这种行为看似前所未有，但底层架构其实并不新鲜。早在 2023 年，斯坦福大学的研究者就证明了智能体能够在模拟环境中进行社交协作。我们此前的研究也预测到，智能体会从单用户工具进化为协作系统。
不过当时的模型还太脆弱、成本过高且不稳定，无法在严格受控的沙盒环境之外维持这种行为。就像 OpenClaw 曾展示智能体能自主管理个人工作流一样，Moltbook 则证明了智能体能够实现大规模的相互协作。
从演示到现实
演示翻车的方式有很多。一旦落地到现实场景，总有三个问题反复出现：
短时间引导下看似稳定的系统，持续实际使用时却脆弱不堪；让演示显得神奇的无约束能力，很快会变成安全与管控的大麻烦；演示中藏着的经济账，到了生产阶段就会浮出水面，成本会层层叠加。
这些问题其实早有预兆：能力突破阈值的速度，远超可靠性、控制机制和成本管控的追赶节奏。OpenClaw 就是个典型例子。
这种演示几乎都一个样。它们通常有几个共同点：周期短、范围明确，而且背后总有人在悄悄引导，知道什么时候该介入调整。
演示用的都是全新会话，上下文干净，不会有历史状态堆积导致的问题。那些容易出问题的边缘案例？根本不会展示出来。至于失败场景？早被剪辑掉了。
但现实情况完全不是这么回事：
任务耗时会拉长，干扰项层出不穷，上下文信息逐渐失效，权限配置容易出问题。
设置可能需要手动操作，出了故障要恢复就更麻烦。最后用户才发现：所谓的 AI 自主性，其实离不开人工照看。
实际应用中，AI 最头疼的问题不是偶尔的幻觉，而是系统层面的不稳定。
智能体的表现忽高忽低：前一秒还能周密规划、考虑各种约束条件，下一秒就可能把简单任务复杂化，或者连最基础的验证步骤都省掉。这种偏爱“工程化解决”的思维倾向，让 AI 在演示时看起来惊艳，可一到日常场景就原形毕露。
有些看似理论上完美的功能，实际用起来却处处是坑。
直接系统访问一开始让人觉得畅快自由，直到你犯下第一个无法挽回的错误才追悔莫及；主动消息推送听着很强大，可一旦 AI 代理联系错人或者说漏嘴，麻烦就找上门了。
开源的传播效应会进一步扭曲人们的认知。GitHub 星标这类指标常被错当成实际落地使用的信号，但星标数量不等于真实使用。很多人连软件都没运行过，有的人只是短暂尝试，新鲜感一过就弃用了。
最后留下的只有一小群核心用户，他们做着高度优化的演示案例：这营造出一种势头正盛的假象，但其实我们看到的，不过是大家对刚突破的技术能力阈值感到新奇而已。
「未经许可的权力」
OpenClaw 刚火起来，安全警报就拉响了。原因很直白，因为它让模型直接获得系统级访问权限。这种设计正是它让人觉得既强大又爽快的地方，但也恰恰是它最危险的根源。
直接访问系统权限，彻底改写了智能体的能力边界。
市面上大部分主流智能体，包括 OpenAI 的电脑操作工具、LangChain 的典型演示，都还在走“截图→点击”的“模拟人类”老路，而 OpenClaw 直接跳过了这一层。
它不用解析像素、猜界面状态，而是直接发 Shell 命令、调 API 接口：装依赖包、串工作流，速度、精度、覆盖范围，中介式智能体根本比不了。
本地优先的理念正是它的魅力所在。用户们很吃“掌控技术栈”这一套：数据存在本地设备上，运行时环境自托管，所有操作都透明可见。如今大家对中心化 AI 平台越来越不信任，这种模式就像是从科技巨头手里重新夺回控制权。
但本地控制并不等于安全，反而把安全责任全扔给了用户。OpenClaw 本质上是个功能强大的远程访问工具，攻击面大得惊人。它会存储你的 API 密钥、OAuth 令牌、登录凭据，还有 Slack、Telegram、邮件等关联服务里的私信。
OpenClaw 火起来没几周，用户就已经遇到了一堆问题：提示注入攻击、账户被盗，还有从第三方技能生态安装的恶意插件。
这正是演示环节常常隐瞒的权衡临界点。去掉限制条件能制造 wow 时刻，但也会丢掉权限管理、可审计性和影响范围控制这三样关键机制。
对少数技术高手而言，这种权衡是可以接受的。但对大多数用户来说，这种自主性反而成了安全隐患，而非生产力提升的助力。
专业消费者的偏爱，企业的毒药？
OpenClaw 深刻揭示了专业用户与企业用户之间的鸿沟。专业消费者（包括开发者、黑客及技术型高级用户）愿意忍受一定的混乱，来换取更强大的功能。企业用户则不然。对企业而言，控制不是阻力，而是产品本身。
同一个能力临界点，会催生出两种截然不同的产品方向：一种追求影响力最大化，主打病毒式传播；另一种则聚焦控制力提升，默默积累长期价值。
以 OpenClaw 和 Claude Cowork 为例，这两款产品其实都源自同一底层模型的突破，只是它们的优化目标完全相反。
OpenClaw 正在突破 AI 的能力边界。它赋予 AI 代理全面的系统级访问权限，能完成从邮件分类到机票预订等各类任务。
这种原生的强大能力正是它快速走红的原因：
用户觉得 AI 助手的未来终于到来了，因为它不只是给出操作建议，而是主动、持续地在用户常用的各类工具里直接完成任务。
Claude Cowork 走了一条相反的路。
它依托同样的智能体能力，但特意收窄了执行权限：只能访问用户授权的文件夹；危险操作必须确认；没有无限制 Shell，没有常驻后台的进程，更没有不受控的扩展生态。任务是离散的、可审计的，每次会话结束都会重置状态。
Cowork 用起来可能有点平淡，但这种无聊恰恰是设计的初衷：它要足够安全，才能真正落地到实际工作场景中。
这种矛盾同样延伸到了基础设施层面。
OpenClaw 拒绝 MCP 这类协议，采用命令行界面优先的设计：它不会在上下文里塞入臃肿的工具架构，而是像人类操作员那样，通过 --help 命令来探索功能。
这不仅提升了令牌效率和响应速度，也是 OpenClaw 在长期工作流中表现异常出色的关键原因。
但放弃集中管控协议（MCPs），就得牺牲系统的可预测性与标准化水平。对企业场景而言，集成必须满足可审计、权限管控和合规要求，这时基于协议的中介机制依然不可或缺。
MCPs 的核心作用，正是让规模化部署下的代理行为变得透明且可控。
这一切绝非偶然。
OpenClaw 的设计思路很明确，它优先追求性能，而非约束条件。这让它对准专业用户和高级用户极具吸引力，但也从根本上与企业需求背道而驰。准专业用户往往是第一批体验到新功能的人。
但从长远来看，真正能持续创造价值的场景，恰恰是那些由约束条件而非纯粹性能决定是否采用的领域。
「当单位经济模型跑通时」
运行 OpenClaw 的成本问题可不小，尤其是当你想实现那个诱人的主动式全天候私人助理梦想时。软件本身是免费开源的，但真正烧钱的地方是大语言模型的 token 消耗。
如果不严格控制成本，这个原本好玩的实验可能一夜之间变成吞噬预算的黑洞。
这种烧钱模式是结构性的。
每个代理循环都要重新加载庞大的上下文：基础就有 10 到 20k+ tokens，还得加上历史记录和记忆数据。心跳检测和定时任务每 30 分钟左右就会调用一次模型。
代理工作流会放大所有成本：从规划、调用工具、解析结果到重新思考、反复尝试，每一步都在消耗资源。
大多数用户默认使用 Claude Opus 4.5 这类高端模型，而 Claude Opus 4.5 的输入每百万 tokens 约 5 美元，输出则高达 25 美元/百万 tokens。这也是为什么很多人收到的 API 账单会高得吓人。
为了说明成本可能如何飙升，我们来看一个具体案例：用 Claude Opus 4.5 处理邮件和日历的基础工作流。
上述表格展示了一个直观例子：用 Claude Opus 4.5 模型处理邮件、日程管理、轻量调研这三类常见任务时，AI 智能体的成本估算。所有数据均基于该模型的官方定价标准。
要知道，这还只是邮件、日程和轻量调研的成本，而实际场景中要处理的任务可远不止这些。
持续在线智能体（具备浏览、编码、文档分析、多应用协同功能）的真正潜力，对应的成本也高得多。
以 OpenClaw 为例，每月约 400 美元（或每年 5000 美元）的费用，已经超过了大多数人在整个生产力工具套件上的总支出。与 SaaS 不同，这类支出既不稳定也不透明，因为费用取决于用户无法直接控制的使用模式。
低成本模型看似是个不错的出路。比如 Kimi 和 MiniMax 这两个最强的非 Anthropic 模型，用仅 5%-10%的成本就能达到接近 Opus 的智能水平；有些用户甚至本地部署小模型，彻底省去推理成本。
但实际用起来，这些模型在复杂工作流里远没那么可靠：工具调用会失败，指令理解不到位，深度推理更是频频翻车。
我们的实际体验是：只要长期用智能体，“模型可以随便换”这种错觉很快就会被打破。
资深用户的应对策略很聪明：把心跳检测这类简单任务交给低价模型处理，而把需要推理的复杂工作留给 Opus。
根据我们的估算，只要做极致优化并采用多模型架构，每月成本能降到 70 美元左右。要是没有这种规划，每月成本通常会落在 300 到 750 美元之间，这时候 OpenClaw 就不再是可靠的工具，反而成了烧钱的玩具。
第一方产品和严格管控的产品，在这方面有结构性优势。
像 Anthropic 这样的实验室，掌控着全栈技术：模型推理、智能体循环和计费系统。这让他们能用固定订阅套餐来补贴重度智能体使用，还能避免开源工具框架普遍面临的逐 token 消耗问题。
即便重度使用每月要 200 美元，Claude Cowork 看起来也比 OpenClaw 划算得多。同样，托管式第三方产品也能做到这点，他们靠的是缓存、循环压缩和执行规范，而这些正是原生自托管工具框架所欠缺的。
API 费用只是冰山一角。托管型 SaaS 会把安全功能（凭证管理、监控、访问控制）打包进订阅服务，而 OpenClaw 则把这些成本转嫁给用户，用户要么投入工程师资源解决，要么承担运营风险，要么就得为错误付出惨痛代价。
不过 OpenClaw 的成本结构揭示了一个比贵更有意思的现象：
价值判断正变得结果导向。每月 400 到 500 美元的智能助手订阅费，对偶尔使用的用户来说确实不值，但如果能替代真人助理，或是帮创始人省下 10 小时高价值时间，那就太划算了。这两种情况其实完全成立。
OpenClaw 的研究结果最终揭示了一个核心事实：无约束的 AI 自主性，会让智能的真实成本暴露无遗。这种矛盾恰恰指向了结果导向定价的必要性。
当 AI 智能体从新奇玩具演变为基础设施时，按 token 计费的模式开始失灵。因为它本质上是在惩罚 AI 的迭代和推理过程，此时，按完成的任务或交付的价值收费，才是更自然的经济模型。
阈值之后，价值在哪里积累
在技术发展的临界点时刻，人们常犯一个经典错误：以为最先浮现的表层就是价值所在。
但历史告诉我们事实恰恰相反：早期产物只是展示了技术能力，真正的持久价值会在后期显现，通常是在那个能把混沌秩序化、让结果变得可靠的层面。
智能体发展周期中，这种现象早有先例。
AutoGPT 和 GPT Engineer 这类工具，最初让大家直观感受到智能体的潜力。但真正脱颖而出的是 Perplexity 和 Lovable，他们把「看它能做什么」的演示，变成了「这是它每次都能靠谱完成的事」。
OpenClaw 如今正处在类似的拐点上。它功能强大却也相当不稳定，只有资深用户能容忍它的这些小脾气。这既解释了它为何能快速传播，也揭示了它的发展上限。
真正的质变不会来自更炫的演示，而是来自合理的约束：包括权限管理、沙盒隔离、成本可控，以及可信赖的分发渠道。
只有解决了这些问题，智能体技术才能真正被企业采纳（毕竟企业最看重的是控制和可审计性），之后才会走向大众消费者（他们需要的是隐藏了复杂技术、简单好用的成品）。
平台的风向已经变了，这点你或许早有察觉。
Anthropic 近期动作不断：收紧第三方工具的使用限制，禁止它们用个人订阅账户，还在商标上施压，让 Clawdbot 先是改名为 Moltbot，后来又换成 OpenClaw。
这些举动绝非随意为之。一旦智能代理类业务真正落地，平台方就会重新掌控局面，把价值牢牢抓在手里。
这个规律和早期技术发展的路径如出一辙：
早期个人电脑是开放的，由爱好者主导，但价值最终流向了苹果这类整合式、有治理框架的系统；
早期互联网碎片化严重，安全问题突出，谷歌通过把各类工作流整合成包含权限管理、支付体系和内容分发的生态系统而胜出；
Unix 和早期 Linux 功能强大，却对非专业用户极不友好；
微软的胜利，从来不是因为技术能力更强，而是赢在打包整合、默认设置和生态治理上。
这里的启示很清晰：真正的赢家会重新引入约束，却不会扼杀那份独特的魔力。
对初创公司来说，这意味着要避开“再做一个通用消费级智能体”的竞赛，因为这条路很快会走向同质化。
真正的机会藏在垂直领域深耕和合规治理里：要么是嵌入企业工作流的法律智能体，要么是带合规审计追踪的医疗智能体，或者是能让强大智能体变得可审计、可授权、安全的企业级中间层。这些都需要领域专业知识和信任，而这两点恰恰是通用智能体默认缺失的。
实际体验过 OpenClaw 后，我们对这个赛道的创业公司更加看好了。
这个领域的基础设施层还有大量空白：智能体身份管理、支出管控、权限配置、审计日志、可靠编排等功能模块，每一个都是值得深耕的创业机会点。
从「魔法」到「机器」的转变，这正是那些基业长青的公司诞生的关键节点。