标题: 生成超级棒MV的Skills，有点Seedance2.0的味道

正文:

当AI可以写歌、画画、剪辑，我们离"一个人的音乐工作室"还有多远？
起因：一个不切实际的想法
故事要从一个深夜说起。
那天我刷到一条短视频，画面是AI生成的动漫风格，配着一首AI写的歌。评论区炸了："这也太酷了吧！"
但我仔细一看，制作流程是这样的：先用ChatGPT写歌词，再用Suno生成音乐，然后用Midjourney一张张画图，最后手动剪辑合成。前前后后折腾了大半天。
我当时就想：
能不能把这些全部串起来，输入一个主题，一键生成完整MV？
作为一个Claude Code的重度用户，我决定把这个想法做成一个skill。
先说结果
经过8个版本的迭代，现在只需要一行命令：
/music2video 童年的冬天 --genre pop --lang zh
20分钟后，你会得到一支完整的竖版MV：
• AI创作的歌词和旋律
• 角色一致的动漫场景插画（20多张）
• 精确到每个字的歌词字幕
• 电影级的转场和运镜效果
• 带封面的竖屏视频，可以直接发短视频平台
整个过程全自动，不需要任何手动操作，请观看下方视频效果。
架构：七个阶段的流水线
做这件事最难的不是单个环节，而是把所有环节串成一条可靠的流水线。
我把整个流程拆成了七个阶段：
歌词创作 → 音乐生成 → 时间戳提取 → 角色设计 → 场景描述 → 图像生成 → 视频合成
每个阶段都由不同的AI模型负责，像一条装配线上的不同工位。
第一站：歌词创作
用大语言模型根据主题写歌词。这里有个关键约束：
歌词必须适合演唱。不能太长（20-30行），不能有太生僻的词，还要有韵律感。
我花了不少时间调prompt，最后发现最有效的方式是给模型明确的"格式约束"而不是"内容约束"——告诉它字数范围、行数限制、不要写注释说明，比告诉它"写得有感情"有用得多。
第二站：音乐生成
歌词写好后，交给音乐生成API。这一步最让我惊喜的是风格提示词的效果。
我设计了一个"六要素公式"来生成风格描述：
比如一首关于童年冬天的歌，生成的风格描述可能是：
Warm nostalgic pop, gentle and dreamy, piano and acoustic guitar, soft childlike female vocal, moderate tempo 95 BPM, East Asian winter atmosphere
效果比简单写"pop"好太多了。为了做好这一步，我整理了1600多条风格参考样本。
第三站：时间戳——让字幕跟上节奏
这是整个项目中最"看不见但最重要"的环节。
音乐生成后，我通过API获取每个字的精确时间戳。拿到的是这样的数据：每个word的开始时间和结束时间，精确到毫秒。
然后我需要把这些单字组合成"行"——用于字幕显示的单位。这里的算法考虑了：
• 英文按词数分组（最多8个词一行）
• 中文按字符数分组（最多14个字一行）
• 单词间超过0.3秒的间隔，视为自然断句点
这样出来的字幕，观感就像KTV的歌词滚动一样自然。
第四站：角色设计——保持一致性的关键
如果每张图的角色长得都不一样，那MV看起来就像PPT而不是动画。
角色一致性是整个项目最大的技术挑战之一。
我的解决方案是：先让LLM设计一个完整的角色描述（发型、服装、配色、体型等），然后生成一张角色参考图。后续所有场景图都会带着这张参考图一起提交给图像生成API。
这样做的效果出奇地好。虽然不是百分之百完美，但角色的发型、服装颜色、体型基本保持一致，足以让观众认出"这是同一个人"。
第五站：场景描述——从歌词到画面
每一句歌词都需要一个对应的画面。但不能简单地"翻译"歌词——需要视觉化思维。
比如歌词是"哈出的白雾气在冬日"，场景描述不能只写"白雾"，而要写出具体的画面：一个穿着红色棉袄的小女孩，站在雪地里，呼出的白气在阳光下像一个小星球。
这一步全靠LLM的"导演能力"。我在prompt里要求它像电影分镜师一样思考：镜头角度、光线方向、情绪氛围、色彩基调。
第六站：图像生成——批量且可靠
一首歌通常有50多句歌词，意味着需要生成50多张图。
这里有两个工程挑战：
速度：串行生成太慢。我采用了3个并发的批处理策略，大幅缩短了等待时间。
可靠性：网络请求有时会失败。V1.8之前，失败的图像会变成纯黑色的占位图——直接毁掉整支MV。
后来我加了一套完整的验证和重试机制：
• 生成完成后，检查每张图的文件大小（真实图像800KB以上，占位图只有5KB）
• 发现占位图，自动重试，最多两轮
• 从缓存加载时也做同样的校验
• 如果重试成功，自动更新缓存
这个改进让成功率从之前的"听天由命"变成了几乎百分之百。
第七站：视频合成——FFmpeg的艺术
最后一步是把所有素材合成视频。这一步代码量最大，也最讲究。
运镜效果：每个画面都不是静止的。我实现了五种运镜效果循环使用——推进、拉远、左移、右移、上移。配合不同的缩放比例模拟广角、中景、特写三种镜头。
智能转场：不同歌曲段落用不同的转场效果。主歌用溶解、淡入淡出（慢节奏），副歌用滑动、擦除（快节奏），桥段和间奏各有各的风格。
分屏和闪切：在副歌高潮部分，约20%的画面会使用分屏效果（两张图并排），10%会使用闪切蒙太奇（快速切换3-5张图）。这些效果让视频的节奏感一下子就上来了。
色彩调色：根据音乐风格自动调色。流行乐偏暖色、摇滚偏高对比度、电子乐偏冷色调。
最后，把音乐、画面、字幕三层叠加在一起。字幕用黑色半透明底框，确保在任何画面上都清晰可读。
迭代的故事：从能用到好用
这个项目经历了8个主要版本。每个版本都解决了一个"看起来小但影响很大"的问题。
其中最有意思的bug是V1.7要解决的问题：AI在生成歌词时，偶尔会在歌词里混入一些"自言自语"，比如"( Wait, removed the note as per rules:"这种。因为歌词会被送去生成时间戳，这些垃圾文本也会被精确打上时间点，最后堂而皇之地出现在字幕里。
解决方案是三层过滤：在歌词生成时加强约束、在时间戳分组后过滤、在字幕渲染时再做一次清洗。
缓存：让迭代成为可能
一次完整生成需要20分钟，其中大部分时间花在音乐生成和图片生成上。如果每次修改字幕样式都要从头来过，那就太崩溃了。
所以我设计了一套缓存机制：
1. 用 MD5(主题+风格+语言)作为缓存key
2. 首次生成后，音乐、歌词时间戳、场景描述、所有图片全部缓存
3. 后续修改只需要重新合成视频，从20分钟缩短到15分钟左右
4. 缓存验证确保不会使用损坏的数据
这个设计让"改一点、试一次"的迭代循环变得可行。
Claude Code Skill的优势
把这个做成Claude Code的skill，而不是独立的Python脚本，有几个关键优势：
对话式交互：用户不需要记住复杂的命令参数，可以用自然语言描述需求。
智能错误处理：出错时，Claude可以分析错误原因并尝试修复，而不是直接报错退出。
增量开发：每次改进都可以直接在现有代码上迭代，Claude Code的git集成让版本管理变得很顺畅。
跨Agent编排：一个流水线里用到了LLM（写歌词、设计角色、描述场景）、音乐生成、图像生成、视频处理四类不同的AI能力，Claude Code天然适合做这种多Agent编排。
写在最后
从第一个版本到现在，这个项目教会我一件事：
AI工具的真正威力不在于单个模型有多强，而在于你能把多少个模型串成一条流水线。
每个AI模型都有自己的强项和局限。LLM擅长理解和创作文本，音乐模型擅长作曲，图像模型擅长绘画。把它们按照正确的顺序组合起来，输入一个简单的主题，输出一支完整的MV——这就是"Skills"的魅力。
当然，这个项目还有很多可以改进的地方。图片偶尔还会出现角色不一致的情况，转场效果还可以更丰富，甚至可以加入AI生成的视频片段来替代静态图片。
但至少现在，你可以用一行命令，在20分钟内得到一部很棒的MV。
这在半年前都是完全不可想象的事情。
我是 Jason，来自「SuperAI编程」。
在评论区告诉我，说不定下一篇，我们就一起共创你的Skills 🚀
如果本文内容对您有启发，欢迎点个【赞】、【在看】或【转发】支持一下；也可以加个
星标
⭐ 一起进入下面群聊，第一时间收到更多实战案例。
