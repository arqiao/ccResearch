标题: 豆包大模型2.0+Claude Skills，3步装好，终于有AI能帮我"看视频做笔记"了！火山引擎全系列可用！
日期: 20260215
原文: https://mp.weixin.qq.com/s/8pVFZQ-NOJKkYtPx7j3IiA

 id="js_content" style="visibility: hidden; opacity: 0; "> 「豆包大模型2.0」，开始帮我"学习"了！ 这是他代替我 去学李沐老师的论文讲解 视频学习效果 把视频给到他，他就能逐帧给我输出学习内容！ 没看懂不要紧，接下来我们还能让AI逐帧详细讲解！ 这才是 AI视频学习新姿势 我们传统的视频总结工具，从来没"看过"视频。 不信你试一下。随便找一个操作教程，用你平时的AI总结工具跑一遍。你会发现，总结里全是"很浅显的内容"。 传统总结效果 背景有什么内容，人物做了什么动作 一个都没说 因为它根本没看画面。 它只是把音频转成了文字，然后总结了一下。 不信换几种视频试试： 操作教程 讲师鼠标移到「文件」菜单，点开「导出」，选了「PDF格式」。 传统工具只写了一句"讲师演示了导出操作"。哪个菜单、哪个选项，一个字没提。 PPT 讲座 幻灯片上的图表、数据、关键结论，全部丢失。 只总结了口述内容。 纯画面视频 一个做菜视频，锅里油冒烟了、菜下锅翻炒、最后装盘摆好。 没有旁白的片段，传统工具直接跳过，总结里一片空白。 传统vs豆包对比 豆包大模型2.0 能真正"看"视频画面。 不用写代码，3步装好，视频总结从此不漏画面。 为什么差这么多 传统工具的原理很简单：提取音频 → 转文字 → 总结。 画面？根本没进入处理流程。 所以讲师说"点击这个按钮"，总结里就只有"点击这个按钮"五个字。哪个按钮、在屏幕什么位置、长什么样——不知道。想知道？自己暂停，一帧帧去看。 那能不能让 AI 先把画面也识别一遍，再和音频合在一起？ 也不行。 想想教程里最常见的一句话：“就是这个”。讲师说"就是这个"的时候，鼠标正指着屏幕上某个按钮。画面和声音必须同时理解，才知道"这个"到底是什么。分开处理再拼接，对不上。 豆包大模型2.0 是多模态原生模型——画面和声音在内部同时处理，不是分步拼接。 传统方式 = AI 只听了个音频，画面得你自己一帧帧暂停去看 豆包大模型2.0 = AI 替你把画面也看了，直接告诉你屏幕上发生了什么 3步，让 AI 真正"看"视频 我把豆包大模型2.0 的视频理解能力做成了一个 Skill。你只需要把视频链接丢给它，剩下的全自动。 第1步：安装 Skill 从 GitHub 下载，放到 Claude Code 的 skills 目录里： https://github.com/Ceeon/video-analyzer-skill 安装Skill 第2步：配置 API Key 去火山引擎开放平台注册一个 API Key（有免费额度），填到 Skill 的配置文件里。一次配好，后面不用再管。 https://console.volcengine.com/ark/region:ark+cn-beijing/model/detail?Id=doubao-seed-2-0-pro 找到API接入入口 API接入入口 复制APIKey 复制APIKey 告诉给 Agent，豆包大模型2.0 的apikey，他就会自己配置好 第3步：丢链接，等结果 在 Claude Code 里直接说： “分析一下这个视频 https://www.bilibili.com/video/BVxxx” 它会自动下载视频、压缩、上传到豆包大模型2.0 分析。 最后返回带时间戳的详细笔记。 B站、YouTube 都支持。 说实话，超预期了 我本来只是想测试一下豆包大模型2.0 的多模态能力。 没想到做出来的效果真的很厉害。 比如我拿一个纯操作教程测试——传统工具只给了一句"讲师演示了操作流程"，而插件列出了每一步点了什么、选了哪个菜单、最终效果是什么样。 以后内容总结，不止能总结文字，也能总结画面。 你给了 AI 一部电影，它只听了配音。现在它终于能看画面了。 
